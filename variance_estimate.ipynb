{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import make_dataloaders\n",
    "from models.vaes import Base, VAE, IWAE, AIS_VAE, ULA_VAE, Stacked_VAE, VAE_with_flows, repeat_data\n",
    "from models.samplers import HMC, MALA, ULA, run_chain\n",
    "import yaml\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from inspect import signature\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "colors = {\n",
    "    0: 'blue',\n",
    "    1: 'red',\n",
    "    2: 'green',\n",
    "    3: 'yellow',\n",
    "    4: 'black',\n",
    "    5: 'orange',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkotelevskii/anaconda3/envs/condatorch/lib/python3.8/site-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/nkotelevskii/anaconda3/envs/condatorch/lib/python3.8/site-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/nkotelevskii/anaconda3/envs/condatorch/lib/python3.8/site-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/nkotelevskii/anaconda3/envs/condatorch/lib/python3.8/site-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = make_dataloaders(dataset='mnist', batch_size=100, val_batch_size=100, binarize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(version):\n",
    "    with open(f'lightning_logs/default/version_{version}/hparams.yaml') as file:\n",
    "        fruits_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        print(fruits_list)\n",
    "        hparams = fruits_list\n",
    "        \n",
    "    path = f'lightning_logs/default/version_{version}/checkpoints/'\n",
    "    file_name = os.listdir(path)[0]\n",
    "    checkpoint = torch.load(f'{path}{file_name}')\n",
    "    \n",
    "    for current_model in [VAE, IWAE, ULA_VAE, AIS_VAE]:\n",
    "        try:\n",
    "            model = current_model(**hparams).to(device)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            print(f'loaded {model.name}')\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 794\n",
    "iwae = load_model(version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transitions_output(model, z, mu, logvar, x):\n",
    "    x = repeat_data(x, model.num_samples)\n",
    "    output = model.run_transitions(z=z,\n",
    "                                    x=x,\n",
    "                                   mu=mu,\n",
    "                                   logvar=logvar)\n",
    "    if len(str(signature(model.loss_function)).split(',')) > 1:\n",
    "        loss = model.loss_function(sum_log_alphas=output[2], sum_log_weights=output[1])\n",
    "    else:\n",
    "        loss = model.loss_function(sum_log_weights=output[1])\n",
    "    import pdb\n",
    "    grad = torch.autograd.grad(loss, model.decoder_net.net[0].bias)[0][:50]\n",
    "    return output, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_W = iwae.decoder_net.net[0].weight.data\n",
    "model_mu = iwae.decoder_net.net[0].bias.data[..., None]\n",
    "\n",
    "C = (model_W @ model_W.T) + (sigma**2) * torch.eye(784, device=device)\n",
    "C_inv = torch.inverse(C)\n",
    "logdetC = torch.logdet(C)\n",
    "\n",
    "first_term = 784 * np.log(2 * np.pi) + logdetC\n",
    "\n",
    "def get_true_loglikelihood(x):\n",
    "    true_loglikelihood = torch.empty(x.shape[0], device=device, dtype=torch.float32)\n",
    "    for i in range(x.shape[0]):\n",
    "        x_cur = x[i].view(784, 1)\n",
    "        S = (x_cur - model_mu) @ (x_cur - model_mu).T\n",
    "        true_loglikelihood[i] = -0.5 * (first_term + torch.trace(C_inv @ S))\n",
    "    return true_loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULA_VAE_reverse(ULA_VAE):\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.reverse_kernels.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30], gamma=0.25)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# NO reverse\n",
    "\n",
    "# ----- ULA_VAE ----- #\n",
    "ula_5 = ULA_VAE(shape=28, act_func=nn.LeakyReLU, num_samples=1, hidden_dim=iwae.hidden_dim, net_type='fc', dataset='mnist',\n",
    "            step_size=0.001, K=5, use_transforms=False, learnable_transitions=False, return_pre_alphas=True, use_score_matching=False,\n",
    "                      ula_skip_threshold=0.1, grad_skip_val=0., grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=False,\n",
    "                     acceptance_rate_target=0.9, annealing_scheme='linear', specific_likelihood='gaussian', sigma=sigma).to(device)\n",
    "ula_5.decoder_net = copy.deepcopy(iwae.decoder_net)\n",
    "for p in ula_5.decoder_net.parameters():\n",
    "    p.requires_grad_(True)\n",
    "ula_5.encoder_net = copy.deepcopy(iwae.encoder_net)\n",
    "ula_5.use_stepsize_update = False\n",
    "\n",
    "# ----- ULA_VAE ----- #\n",
    "ula_10 = ULA_VAE(shape=28, act_func=nn.LeakyReLU, num_samples=1, hidden_dim=iwae.hidden_dim, net_type='fc', dataset='mnist',\n",
    "            step_size=0.001, K=10, use_transforms=False, learnable_transitions=False, return_pre_alphas=True, use_score_matching=False,\n",
    "                      ula_skip_threshold=0.1, grad_skip_val=0., grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=False,\n",
    "                     acceptance_rate_target=0.9, annealing_scheme='linear', specific_likelihood='gaussian', sigma=sigma).to(device)\n",
    "ula_10.decoder_net = copy.deepcopy(iwae.decoder_net)\n",
    "for p in ula_10.decoder_net.parameters():\n",
    "    p.requires_grad_(True)\n",
    "ula_10.encoder_net = copy.deepcopy(iwae.encoder_net)\n",
    "ula_10.use_stepsize_update = False\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Reverse\n",
    "\n",
    "# ----- ULA_VAE ----- #\n",
    "ula_5_r = ULA_VAE_reverse(shape=28, act_func=nn.LeakyReLU, num_samples=1, hidden_dim=iwae.hidden_dim, net_type='fc', dataset='mnist',\n",
    "            step_size=0.001, K=5, use_transforms=False, learnable_transitions=False, return_pre_alphas=True, use_score_matching=False,\n",
    "                      ula_skip_threshold=0.1, grad_skip_val=0., grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=False,\n",
    "                     acceptance_rate_target=0.9, annealing_scheme='linear', specific_likelihood='gaussian', sigma=sigma, use_reverse_kernel=True).to(device)\n",
    "ula_5_r.decoder_net = copy.deepcopy(iwae.decoder_net)\n",
    "for p in ula_5_r.decoder_net.parameters():\n",
    "    p.requires_grad_(True)\n",
    "ula_5_r.encoder_net = copy.deepcopy(iwae.encoder_net)\n",
    "ula_5_r.use_stepsize_update = False\n",
    "\n",
    "\n",
    "# ----- ULA_VAE ----- #\n",
    "ula_10_r = ULA_VAE_reverse(shape=28, act_func=nn.LeakyReLU, num_samples=1, hidden_dim=iwae.hidden_dim, net_type='fc', dataset='mnist',\n",
    "            step_size=0.001, K=10, use_transforms=False, learnable_transitions=False, return_pre_alphas=True, use_score_matching=False,\n",
    "                      ula_skip_threshold=0.1, grad_skip_val=0., grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=False,\n",
    "                     acceptance_rate_target=0.9, annealing_scheme='linear', specific_likelihood='gaussian', sigma=sigma, use_reverse_kernel=True).to(device)\n",
    "ula_10_r.decoder_net = copy.deepcopy(iwae.decoder_net)\n",
    "for p in ula_10_r.decoder_net.parameters():\n",
    "    p.requires_grad_(True)\n",
    "ula_10_r.encoder_net = copy.deepcopy(iwae.encoder_net)\n",
    "ula_10_r.use_stepsize_update = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# No multisample\n",
    "\n",
    "# ----- AIS_VAE ----- #\n",
    "ais_5 = AIS_VAE(shape=28, act_func=nn.LeakyReLU, num_samples=1, hidden_dim=iwae.hidden_dim, net_type='fc', dataset='mnist',\n",
    "                  step_size=0.01, K=5, use_barker=False, learnable_transitions=False, use_alpha_annealing=True, grad_skip_val=0.,\n",
    "                      grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=False,\n",
    "                     acceptance_rate_target=0.8, annealing_scheme='linear', specific_likelihood='gaussian', sigma=sigma).to(device)\n",
    "ais_5.decoder_net = copy.deepcopy(iwae.decoder_net)\n",
    "for p in ais_5.decoder_net.parameters():\n",
    "    p.requires_grad_(True)\n",
    "ais_5.encoder_net = copy.deepcopy(iwae.encoder_net)\n",
    "ais_5.use_stepsize_update = False\n",
    "\n",
    "\n",
    "# ----- AIS_VAE ----- #\n",
    "ais_10 = AIS_VAE(shape=28, act_func=nn.LeakyReLU, num_samples=1, hidden_dim=iwae.hidden_dim, net_type='fc', dataset='mnist',\n",
    "                  step_size=0.01, K=10, use_barker=False, learnable_transitions=False, use_alpha_annealing=True, grad_skip_val=0.,\n",
    "                      grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=False,\n",
    "                     acceptance_rate_target=0.8, annealing_scheme='linear', specific_likelihood='gaussian', sigma=sigma).to(device)\n",
    "ais_10.decoder_net = copy.deepcopy(iwae.decoder_net)\n",
    "for p in ais_10.decoder_net.parameters():\n",
    "    p.requires_grad_(True)\n",
    "ais_10.encoder_net = copy.deepcopy(iwae.encoder_net)\n",
    "ais_10.use_stepsize_update = False\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Multisample\n",
    "\n",
    "# ----- AIS_VAE ----- #\n",
    "ais_5_3 = AIS_VAE(shape=28, act_func=nn.LeakyReLU, num_samples=3, hidden_dim=iwae.hidden_dim, net_type='fc', dataset='mnist',\n",
    "                  step_size=0.01, K=5, use_barker=False, learnable_transitions=False, use_alpha_annealing=True, grad_skip_val=0.,\n",
    "                      grad_clip_val=0., use_cloned_decoder=False, variance_sensitive_step=False,\n",
    "                     acceptance_rate_target=0.8, annealing_scheme='linear', specific_likelihood='gaussian', sigma=sigma).to(device)\n",
    "ais_5_3.decoder_net = copy.deepcopy(iwae.decoder_net)\n",
    "for p in ais_5_3.decoder_net.parameters():\n",
    "    p.requires_grad_(True)\n",
    "ais_5_3.encoder_net = copy.deepcopy(iwae.encoder_net)\n",
    "ais_5_3.use_stepsize_update = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(model):\n",
    "    for batch in tqdm(val_loader):\n",
    "        x, _ = batch\n",
    "        x = x.to(device)\n",
    "        z, mu, logvar = iwae.enc_rep(x, model.num_samples) # <- latents are fixed\n",
    "\n",
    "        model_w = torch.tensor([], device=device, dtype=torch.float32)\n",
    "        model_g = []\n",
    "\n",
    "        true_loglikelihood = get_true_loglikelihood(x).repeat(model.num_samples).cpu().detach().numpy()\n",
    "        true_loglikelihood_mean = np.mean(true_loglikelihood)\n",
    "        for i in range(n):\n",
    "            model_log_w, grad_model = get_transitions_output(model, z, mu, logvar, x)\n",
    "            with torch.no_grad():\n",
    "                model_log_w = model_log_w[1]\n",
    "                model_w = torch.cat([model_w, model_log_w[..., None]], dim=1)\n",
    "                model_g.append(grad_model.cpu().detach().numpy())\n",
    "    \n",
    "    \n",
    "    return model_w, true_loglikelihood_mean, np.array(model_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trainer(model):\n",
    "    tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/')\n",
    "    trainer = pl.Trainer(logger=tb_logger, fast_dev_run=False, max_epochs=101, automatic_optimization=True, gpus=1)\n",
    "    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trainer(ula_5_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trainer(ula_10_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ula_5_l2, ula_5_grad = run_exp(ula_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ula_10_l2, ula_10_grad = run_exp(ula_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_5_l2, ais_5_grad = run_exp(ais_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_10_l2, ais_10_grad = run_exp(ais_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_5_5_l2, ais_5_5_grad = run_exp(ais_5_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_5_5_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ula_5_l2, ula_5_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ula_10_l2, ula_10_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_5_l2, ais_5_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_10_l2, ais_10_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
